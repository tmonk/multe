This derivation adapts @ophem1999multichoice, and @ben1985discrete [pp
104-107].\

# Additive random utility model

The agent is presented with a normalised set of $J$ options. She is
instructed to pick the $j_1$ most important options from that set. In
what follows, we consider the case $j_1=2$, but the derivation can be
extended to higher $j_1$.

Let there be $J$ alternatives, indexed by
$j\in\mathcal{J}=\{1,\dots,J\}$. The utility generated by alternative
$j$ for a given individual (individual subscripts are suppressed for
simplicity) is $$\begin{equation}
U_j = V_j + \epsilon_j,
\end{equation}$$ where $V_j$ is the deterministic component of utility,
depending on individual and alternative specific characteristics, and
$\epsilon_j$ is a stochastic component.

We assume that the disturbances $\epsilon_j$ are independently and
identically distributed across $j$ and across individuals, following a
type-I extreme value (Gumbel) distribution with CDF $$\begin{equation}
\label{eq:gumbel-base}
F_\epsilon(\epsilon) = \exp\bigl(-\exp(-\epsilon)\bigr).
\end{equation}$$ Under this assumption, the CDF of $U_j$ is
$$\begin{equation}
\label{eq:Uj-cdf}
F_{U_j}(u) = \Pr(U_j \le u) 
  = \Pr(\epsilon_j \le u - V_j)
  = \exp\bigl(-\exp(-(u - V_j))\bigr),
\end{equation}$$ that is, $U_j$ is Gumbel with location parameter $V_j$
and unit scale.

We are interested in the probability that two specified alternatives,
say $s$ and $t$, are jointly among the two most preferred alternatives:
that is, the event that both $U_s$ and $U_t$ exceed the utilities of all
remaining alternatives.

Define $$U^* \equiv \max_{j\notin\{s,t\}} U_j.$$ The event of interest
can be written as $$\begin{equation}
\label{eq:pair-event}
E_{s,t} 
= \{U_s > U^*,\, U_t > U^*\}.
\end{equation}$$ Equivalently, $$E_{s,t} 
= \bigl\{ \min(U_s,U_t) > U^* \bigr\}.$$

To use inclusion--exclusion arguments, define
$$A = \{U_s \le U^*\}, \qquad B = \{U_t \le U^*\}.$$ Then
$E_{s,t} = A^c \cap B^c$ and therefore $$\begin{align}
E_{s,t}^c 
&= A\cup B, \\
P(E_{s,t}) 
&= 1 - P(A\cup B)
 = 1 - \bigl[P(A) + P(B) - P(A\cap B)\bigr], \label{eq:key-eq}
\end{align}$$ where $$P(A) = P(U_s \le U^*),\quad
P(B) = P(U_t \le U^*),\quad
P(A\cap B) = P\bigl(\max\{U_s,U_t\} \le U^*\bigr).$$

## Single choice made

In the case where only one option is chosen (e.g. "None" or "Don't
know"), we need only the simple single-choice probability for
alternative $1$: $$\begin{equation}
\label{one-choice-prob}
P\Bigl(U_1 > \max_{j=2,\dots,J} U_j\Bigr),
\end{equation}$$ which will be derived explicitly in
Section [1.3.3](#subsec:single-choice-probability){reference-type="ref"
reference="subsec:single-choice-probability"} below.

## Properties of the Gumbel distribution

If the random variable $\epsilon$ is distributed under a general Gumbel
distribution, then its CDF can be written as $$\begin{equation}
F(\epsilon) = \exp\bigl(-\exp(-\mu(\epsilon - \eta))\bigr),
\end{equation}$$ where $\eta$ is a location parameter and $\mu>0$ is a
scale parameter. The distribution has the following properties:

1.  The mode is $\eta$.

2.  The mean is $\eta + \gamma/\mu$, where $\gamma$ is Euler's constant.

3.  The variance is $\pi^2/(6\mu^2)$.

4.  []{#affine-gumbel label="affine-gumbel"} *Affine transformation:* If
    $\epsilon$ is Gumbel distributed with parameters $(\eta,\mu)$ and
    $\alpha>0$, $V$ are scalar constants, then
    $$\alpha \epsilon + V \sim \text{Gumbel}(\alpha\eta+V,\mu/\alpha).$$

5.  []{#sum-gumbel label="sum-gumbel"} *Difference of two Gumbels:* If
    $\epsilon_1$ and $\epsilon_2$ are independent Gumbel variables with
    parameters $(\eta_1,\mu)$ and $(\eta_2,\mu)$ respectively, then
    $$\epsilon^\star = \epsilon_1 - \epsilon_2$$ is logistically
    distributed with location $\eta_1 - \eta_2$ and scale $1/\mu$. Its
    CDF is $$\begin{equation}
      \label{distribution-gumbel}
      F_{\epsilon^\star}(z) =
        \frac{1}{1 + \exp\bigl(-\mu (z - (\eta_1 - \eta_2))\bigr)}.
      
    \end{equation}$$ Equivalently,
    $$F_{\epsilon^\star}(z) = \frac{1}{1 + \exp\bigl(\mu(\eta_2 - \eta_1 - z)\bigr)}.$$

6.  []{#max-gumbel-prop label="max-gumbel-prop"} *Max of Gumbels:* If
    $(\epsilon_1,\dots,\epsilon_J)$ are independent Gumbel random
    variables with parameters $(\eta_j,\mu)$ respectively, then
    $$\max(\epsilon_1,\dots,\epsilon_J)$$ is Gumbel distributed with
    parameters $$\begin{equation}
      \label{max-gumbel}
      \left(
        \frac{1}{\mu}\ln\sum_{j=1}^J \exp(\mu\eta_j),\,
        \mu
      \right).
      
    \end{equation}$$

The mean of $\epsilon_j$ is not identified if $V_j$ contains an
intercept. We can therefore, without loss of generality, impose that
$\eta_j = 0$ for all $j$.

More generally, the overall *scale* of utility is not identified: if
$\{U_j\}$ and $\{c U_j\}$, for any $c>0$, are both feasible, they
generate the same choice probabilities. Consequently, only the ratios of
coefficients to the error scale are identified. A common normalisation
is to fix the Gumbel scale at $\mu=1$ and set the location parameters to
zero, $\eta_j=0$ for all $j$. Under this normalisation,
$$\epsilon_j \sim \text{Gumbel}(0,1), \qquad U_j = V_j + \epsilon_j.$$

With this normalisation, the difference of two independent Gumbels
simplifies to a standard logistic distribution. In particular, equation
[\[distribution-gumbel\]](#distribution-gumbel){reference-type="eqref"
reference="distribution-gumbel"} reduces to $$\begin{equation}
\tag{\ref{distribution-gumbel}a}
F_{\epsilon^\star}(z)
= \frac{1}{1+\exp(-z)},
\end{equation}$$ i.e. $\epsilon^\star$ is logistic with unit scale and
zero location.

## Deriving the multichoice logit model

Under the normalisation above, we have
$\epsilon_j\sim\text{Gumbel}(0,1)$ and therefore
$$U_j = V_j + \epsilon_j \sim \text{Gumbel}(V_j,1).$$

### Distributions of maxima and differences

Fix two distinct alternatives $s$ and $t$. Define
$$U^* = \max_{j\notin\{s,t\}} U_j.$$ Using
property [\[max-gumbel-prop\]](#max-gumbel-prop){reference-type="ref"
reference="max-gumbel-prop"}, $U^*$ is also Gumbel distributed:
$$\begin{equation}
\label{eq:max-U-star}
U^* \sim \text{Gumbel}\Bigl(\ln\sum_{j\notin\{s,t\}} e^{V_j},\,1\Bigr).
\end{equation}$$ Similarly, the maximum of $U_s$ and $U_t$,
$$U^{\max}_{st} \equiv \max\{U_s,U_t\},$$ is Gumbel distributed with
parameters $$\begin{equation}
\label{eq:max-U-st}
U^{\max}_{st} \sim \text{Gumbel}\Bigl(\ln\bigl(e^{V_s}+e^{V_t}\bigr),\,1\Bigr).
\end{equation}$$

Now define the differences $$\begin{equation}
\label{eq:Z-defs}
Z_s = U^* - U_s,\qquad
Z_t = U^* - U_t,\qquad
Z_{st} = U^* - U^{\max}_{st}.
\end{equation}$$ By
property [\[sum-gumbel\]](#sum-gumbel){reference-type="ref"
reference="sum-gumbel"}, each of these is logistic, with:

- $Z_s$ having location
  $$\delta_s = \ln\sum_{j\notin\{s,t\}}e^{V_j} - V_s,$$

- $Z_t$ having location
  $$\delta_t = \ln\sum_{j\notin\{s,t\}}e^{V_j} - V_t,$$

- $Z_{st}$ having location
  $$\delta_{st} = \ln\sum_{j\notin\{s,t\}}e^{V_j} - \ln\bigl(e^{V_s}+e^{V_t}\bigr).$$

In each case, the scale is 1, so the CDF is
$$F_{Z}(z) = \frac{1}{1+\exp\bigl(-(z-\delta)\bigr)}.$$

### Computing the pairwise probability

Introduce the shorthand $$a_j = e^{V_j},\qquad
R \equiv \sum_{j\notin\{s,t\}} a_j.$$

From the previous subsection,
$$\delta_s = \ln R - V_s \quad\Rightarrow\quad \exp(\delta_s) = \frac{R}{a_s}.$$
Therefore $$F_{Z_s}(0) = \frac{1}{1 + \exp(\delta_s)}
           = \frac{1}{1 + R/a_s}
           = \frac{a_s}{a_s + R}.$$ Since $Z_s = U^* - U_s$, we have
$$\begin{equation}
\label{eq:Ustar-le-Us}
P(U^* \le U_s) 
= P(Z_s \le 0) 
= \frac{a_s}{a_s+R}.
\end{equation}$$ Hence $$\begin{equation}
\label{eq:Us-le-Ustar}
P(U_s \le U^*) = 1 - P(U^* \le U_s)
               = \frac{R}{a_s+R}.
\end{equation}$$ By symmetry, $$\begin{equation}
\label{eq:Ut-le-Ustar}
P(U_t \le U^*) = \frac{R}{a_t+R}.
\end{equation}$$

For the joint event in
[\[eq:key-eq\]](#eq:key-eq){reference-type="eqref"
reference="eq:key-eq"}, note that $$\max\{U_s,U_t\} \le U^*
\iff U^{\max}_{st} \le U^*.$$ Using $Z_{st} = U^* - U^{\max}_{st}$ and
the same logistic argument,
$$F_{Z_{st}}(0) = \frac{1}{1 + \exp(\delta_{st})}
              = \frac{1}{1+R/(a_s+a_t)}
              = \frac{a_s+a_t}{a_s+a_t+R},$$ so $$\begin{equation}
\label{eq:Umax-st-le-Ustar}
P\bigl(\max\{U_s,U_t\} \le U^*\bigr)
= P(U^{\max}_{st} \le U^*)
= 1 - F_{Z_{st}}(0)
= \frac{R}{a_s+a_t+R}.
\end{equation}$$

Substituting
[\[eq:Us-le-Ustar\]](#eq:Us-le-Ustar){reference-type="eqref"
reference="eq:Us-le-Ustar"},
[\[eq:Ut-le-Ustar\]](#eq:Ut-le-Ustar){reference-type="eqref"
reference="eq:Ut-le-Ustar"}, and
[\[eq:Umax-st-le-Ustar\]](#eq:Umax-st-le-Ustar){reference-type="eqref"
reference="eq:Umax-st-le-Ustar"} into the inclusion--exclusion identity
[\[eq:key-eq\]](#eq:key-eq){reference-type="eqref"
reference="eq:key-eq"}, we obtain $$\begin{align}
P(E_{s,t})
&= 1 - \left[
   \frac{R}{a_s+R} 
 + \frac{R}{a_t+R} 
 - \frac{R}{a_s+a_t+R}
\right] \nonumber\\[0.5em]
&= 1 - \frac{R}{a_s+R} - \frac{R}{a_t+R} + \frac{R}{a_s+a_t+R}.
\label{eq:dual-choice-prob-R}
\end{align}$$

Equivalently, using $a_s/(a_s+R) = 1 - R/(a_s+R)$ and similarly for $t$
and $(s,t)$, we can write the same probability as $$\begin{equation}
\label{eq:dual-choice-prob-as}
P(E_{s,t})
= \frac{a_s}{a_s+R} 
+ \frac{a_t}{a_t+R}
- \frac{a_s+a_t}{a_s+a_t+R}.
\end{equation}$$ Both forms
[\[eq:dual-choice-prob-R\]](#eq:dual-choice-prob-R){reference-type="eqref"
reference="eq:dual-choice-prob-R"} and
[\[eq:dual-choice-prob-as\]](#eq:dual-choice-prob-as){reference-type="eqref"
reference="eq:dual-choice-prob-as"} are algebraically equivalent;
[\[eq:dual-choice-prob-R\]](#eq:dual-choice-prob-R){reference-type="eqref"
reference="eq:dual-choice-prob-R"} is close to the inclusion--exclusion
expression, while
[\[eq:dual-choice-prob-as\]](#eq:dual-choice-prob-as){reference-type="eqref"
reference="eq:dual-choice-prob-as"} is more directly interpretable in
logit terms.

### Single-choice probability {#subsec:single-choice-probability}

Returning to the single-choice case
[\[one-choice-prob\]](#one-choice-prob){reference-type="eqref"
reference="one-choice-prob"}, define $$U^*_1 = \max_{j=2,\dots,J} U_j,$$
and consider $$E_1 = \{U_1 > U^*_1\}.$$ As before, define
$Z_1 = U^*_1 - U_1$. Using the same difference-of-Gumbels logic, $Z_1$
is logistic with location $$\delta_1 = \ln\sum_{j=2}^J e^{V_j} - V_1.$$
Thus $$P(U^*_1 \le U_1) 
= P(Z_1 \le 0)
= \frac{1}{1+\exp(\delta_1)}
= \frac{e^{V_1}}{e^{V_1} + \sum_{j=2}^J e^{V_j}}.$$ Since
$U^*_1 = \max_{j=2,\dots,J}U_j$, this is exactly
$$P\Bigl(U_1 > \max_{j=2,\dots,J} U_j\Bigr).$$ Therefore, the correct
single-choice logit probability is $$\begin{equation}
\tag{\ref{one-choice-prob}a}
P\Bigl(U_1 > \max_{j=2,\dots,J} U_j\Bigr)
= \frac{e^{V_1}}{e^{V_1} + \sum_{j=2}^J e^{V_j}}
= \frac{e^{V_1}}{\sum_{j=1}^J e^{V_j}}.
\end{equation}$$ The complementary probability, that some other
alternative has utility at least as large as $U_1$, is
$$P\Bigl(\max_{j=2,\dots,J} U_j \ge U_1\Bigr)
= \frac{\sum_{j=2}^J e^{V_j}}{\sum_{j=1}^J e^{V_j}}.$$ In the
single-choice part of the likelihood we must use the former (with
$e^{V_s}$ in the numerator), not the latter.

## The likelihood function

Assume the deterministic part of utility for alternative $j$ is a linear
function of parameters, $$\begin{equation}
V_{ij} = X_i^\prime \beta_j,
\end{equation}$$ where $X_i$ is a $k$-vector of attributes for
individual $i$, and $\beta_j$ is a $k$-vector of alternative-specific
parameters.

We now construct the likelihood for the case where respondents may
either choose a *pair* of alternatives (dual choice) or a *single*
alternative.

### Probabilities

Define $$a_{ij} = \exp(V_{ij}),\qquad
R_{i,s,t} = \sum_{j\notin\{s,t\}} a_{ij}.$$

For an observation $i$ that chooses the unordered pair $(s,t)$, the
probability is $$\begin{equation}
\label{eq:Pst}
P_{i,s,t}
= 1 - \frac{R_{i,s,t}}{a_{is} + R_{i,s,t}}
    - \frac{R_{i,s,t}}{a_{it} + R_{i,s,t}}
    + \frac{R_{i,s,t}}{a_{is} + a_{it} + R_{i,s,t}}.
\end{equation}$$ Equivalently, $$\begin{equation}
\label{dual-choices}
P_{i,s,t}
= \frac{a_{is}}{a_{is}+R_{i,s,t}}
  + \frac{a_{it}}{a_{it}+R_{i,s,t}}
  - \frac{a_{is}+a_{it}}{a_{is}+a_{it}+R_{i,s,t}}.
\end{equation}$$

For an observation $i$ that chooses a single alternative $s$
(e.g. "None" or "Don't know" or any other single choice), the
probability is the usual MNL probability $$\begin{equation}
\label{eq:Ps-single}
P_{i,s}^{(1)}
= \frac{a_{is}}{\sum_{j=1}^J a_{ij}}
= \frac{\exp(V_{is})}{\sum_{j=1}^J \exp(V_{ij})}.
\end{equation}$$

### Indicators and likelihood

Define indicator variables for dual and single choices:

- For each individual $i$ and unordered pair $(s,t)$ with
  $1\le s<t\le J$, let $d_{i,s,t}=1$ if individual $i$ chooses the pair
  $(s,t)$ and $0$ otherwise.

- For each individual $i$ and alternative $s$, let $d_{i,s}=1$ if
  individual $i$ chooses $s$ as a single alternative and $0$ otherwise.

For the full dataset of $N$ observations (allowing both dual and single
choices), the contribution of observation $i$ to the likelihood is
$$L_i(\beta) 
= \prod_{1\le s<t\le J} P_{i,s,t}^{\,d_{i,s,t}}
  \prod_{s=1}^{J} \left(P_{i,s}^{(1)}\right)^{d_{i,s}}.$$ The full
likelihood is $$\begin{equation}
\label{eq:likelihood-full}
L(\beta) = \prod_{i=1}^N L_i(\beta)
= \prod_{i=1}^N
    \left[
      \prod_{1\le s<t\le J} P_{i,s,t}^{\,d_{i,s,t}}
      \prod_{s=1}^{J} \left(P_{i,s}^{(1)}\right)^{d_{i,s}}
    \right],
\end{equation}$$ and the log-likelihood is $$\begin{equation}
\label{eq:loglik-full}
\ell(\beta)
= \log L(\beta)
= \sum_{i=1}^N \left[
      \sum_{1\le s<t\le J} d_{i,s,t} \log P_{i,s,t}
      + \sum_{s=1}^{J} d_{i,s} \log P_{i,s}^{(1)}
  \right].
\end{equation}$$

### Using dual choices only

In some applications we may wish to use only the subset of the data in
which each agent chooses exactly two options. Let $N_d$ be the number of
such observations, and let $S_d=\{(s,t):1\le s<t\le D\}$ denote the set
of all unordered pairs for the $D$ alternatives considered in this
reduced sample.

For this dual-choice-only sample, the log-likelihood simplifies to
$$\begin{equation}
\label{eq:loglik-dual-only}
\ell(\beta)
= \sum_{i=1}^{N_d} \sum_{(s,t)\in S_d} d_{i,s,t}\,\log P_{i,s,t},
\end{equation}$$ with $P_{i,s,t}$ given by
[\[dual-choices\]](#dual-choices){reference-type="eqref"
reference="dual-choices"}.

## Maximum Likelihood Estimation

The maximum likelihood estimator is obtained by maximising the
log-likelihood with respect to $\beta$. For the dual-choice-only case,
using
[\[eq:loglik-dual-only\]](#eq:loglik-dual-only){reference-type="eqref"
reference="eq:loglik-dual-only"}, we have $$\begin{equation}
\hat\beta
= \arg\max_{\beta} \ell(\beta)
= \arg\max_{\beta} 
  \sum_{i=1}^{N_d} \sum_{(s,t)\in S_d} d_{i,s,t}\,\log P_{i,s,t},
\end{equation}$$ where $P_{i,s,t}$ is defined in
[\[dual-choices\]](#dual-choices){reference-type="eqref"
reference="dual-choices"}. For the full sample, we use the
log-likelihood in
[\[eq:loglik-full\]](#eq:loglik-full){reference-type="eqref"
reference="eq:loglik-full"}, with the restriction that, for each $i$,
$$\begin{equation}
\sum_{1 \le s < t \le J} d_{i,s,t} + \sum_{s=1}^J d_{i,s} = 1.
\end{equation}$$

## Jacobian (gradient of the log-likelihood) {#subsec:jacobian}

We derive the gradient (Jacobian) of the log-likelihood with respect to
the parameters. It is informative to first obtain the derivatives with
respect to the deterministic utilities $V_{ij}$ and then apply the chain
rule.

Consider a single observation $i$ that chooses the unordered pair
$(s,t)$. Its log-likelihood contribution is $$\ell_i = \log P_{i,s,t}.$$
Recall the notation $$a_{ij} = e^{V_{ij}},\qquad
R_{i,s,t} = \sum_{j\notin\{s,t\}} a_{ij}.$$ For simplicity in this
subsection, we suppress the observation index $i$ in the notation and
write $a_j$, $R$, $P_{s,t}$ instead of $a_{ij}$, $R_{i,s,t}$,
$P_{i,s,t}$.

From [\[dual-choices\]](#dual-choices){reference-type="eqref"
reference="dual-choices"}, $$P_{s,t}
= \frac{a_s}{a_s+R}
  + \frac{a_t}{a_t+R}
  - \frac{a_s+a_t}{a_s+a_t+R}.$$

We now compute $\partial P_{s,t}/\partial V_j$ for $j=s$, $j=t$, and
$j\notin\{s,t\}$.

#### Derivative with respect to $V_s$.

Using $a_s = e^{V_s}$ and $R=\sum_{j\notin\{s,t\}}a_j$, we have
$$\frac{\partial a_s}{\partial V_s} = a_s,\qquad
\frac{\partial R}{\partial V_s} = 0.$$ Only the first and third terms in
$P_{s,t}$ depend on $a_s$. Differentiating, $$\begin{align*}
\frac{\partial}{\partial V_s}\left(\frac{a_s}{a_s+R}\right)
&= \frac{(a_s+R)\,a_s - a_s\,a_s}{(a_s+R)^2}
 = \frac{a_s R}{(a_s+R)^2},\\
\frac{\partial}{\partial V_s}\left(\frac{a_s+a_t}{a_s+a_t+R}\right)
&= \frac{(a_s+a_t+R)\,a_s - (a_s+a_t)\,a_s}{(a_s+a_t+R)^2}
 = \frac{a_s R}{(a_s+a_t+R)^2}.
\end{align*}$$ Therefore $$\begin{equation}
\label{eq:dP_dVs}
\frac{\partial P_{s,t}}{\partial V_s}
= a_s R\left[
    \frac{1}{(a_s+R)^2}
  - \frac{1}{(a_s+a_t+R)^2}
  \right].
\end{equation}$$

#### Derivative with respect to $V_t$.

By symmetry, $$\begin{equation}
\label{eq:dP_dVt}
\frac{\partial P_{s,t}}{\partial V_t}
= a_t R\left[
    \frac{1}{(a_t+R)^2}
  - \frac{1}{(a_s+a_t+R)^2}
  \right].
\end{equation}$$

#### Derivative with respect to $V_r$ for $r\notin\{s,t\}$.

For $r\notin\{s,t\}$, the dependence is via $R$ only. Using
$$\frac{\partial a_r}{\partial V_r} = a_r,\qquad
\frac{\partial R}{\partial V_r} = a_r,$$ and differentiating each term
in $P_{s,t}$ with respect to $R$, we obtain $$\begin{align*}
\frac{\partial}{\partial R}\left(\frac{a_s}{a_s+R}\right)
&= -\frac{a_s}{(a_s+R)^2},\\
\frac{\partial}{\partial R}\left(\frac{a_t}{a_t+R}\right)
&= -\frac{a_t}{(a_t+R)^2},\\
\frac{\partial}{\partial R}\left(\frac{a_s+a_t}{a_s+a_t+R}\right)
&= -\frac{a_s+a_t}{(a_s+a_t+R)^2}.
\end{align*}$$ Therefore, $$\begin{equation}
\label{eq:dP_dVr}
\frac{\partial P_{s,t}}{\partial V_r}
= a_r\left[
    -\frac{a_s}{(a_s+R)^2}
    -\frac{a_t}{(a_t+R)^2}
    +\frac{a_s+a_t}{(a_s+a_t+R)^2}
  \right],\quad r\notin\{s,t\}.
\end{equation}$$

#### Log-likelihood derivatives with respect to $V_j$.

The derivative of the log-likelihood contribution from observation $i$
is $$\frac{\partial \ell_i}{\partial V_j}
= \frac{1}{P_{s,t}}\frac{\partial P_{s,t}}{\partial V_j},$$ for
$j=1,\dots,J$, using the appropriate expression from
[\[eq:dP_dVs\]](#eq:dP_dVs){reference-type="eqref"
reference="eq:dP_dVs"},
[\[eq:dP_dVt\]](#eq:dP_dVt){reference-type="eqref"
reference="eq:dP_dVt"}, or
[\[eq:dP_dVr\]](#eq:dP_dVr){reference-type="eqref"
reference="eq:dP_dVr"} depending on whether $j=s$, $j=t$, or
$j\notin\{s,t\}$.

Importantly, even for $j\notin\{s,t\}$, the derivative
$\partial\ell_i/\partial V_j$ is generally non-zero because the
probability $P_{s,t}$ depends on all alternatives via the sum $R$.

#### From utilities to parameters.

With the specification $$V_{ij} = X_i^\prime \beta_j,$$ we have
$$\frac{\partial V_{ij}}{\partial \beta_j} = X_i,\qquad
\frac{\partial V_{ij}}{\partial \beta_{j'}} = 0 \quad\text{for } j'\neq j.$$
Thus, for each alternative $j$, $$\begin{equation}
\label{eq:dell_dbeta_j}
\frac{\partial \ell(\beta)}{\partial \beta_j}
= \sum_{i=1}^{N_d} 
   \sum_{(s,t)\in S_d}
   d_{i,s,t}\,
   \frac{1}{P_{i,s,t}}
   \frac{\partial P_{i,s,t}}{\partial V_{ij}}\,
   X_i,
\end{equation}$$ where $\partial P_{i,s,t}/\partial V_{ij}$ is given by
the appropriate version of
[\[eq:dP_dVs\]](#eq:dP_dVs){reference-type="eqref"
reference="eq:dP_dVs"},
[\[eq:dP_dVt\]](#eq:dP_dVt){reference-type="eqref"
reference="eq:dP_dVt"}, or
[\[eq:dP_dVr\]](#eq:dP_dVr){reference-type="eqref"
reference="eq:dP_dVr"}, with $a_{ij} = \exp(V_{ij})$ and
$R_{i,s,t} = \sum_{k\notin\{s,t\}} a_{ik}$.

Equation [\[eq:dell_dbeta_j\]](#eq:dell_dbeta_j){reference-type="eqref"
reference="eq:dell_dbeta_j"} defines the Jacobian (gradient) of the
log-likelihood with respect to the parameter vectors $\beta_j$. In
practice, this gradient can be used with standard numerical optimisation
algorithms (e.g. BFGS) to obtain the maximum likelihood estimates.

## Issues and extensions: single vs. dual choices

The model derived above naturally applies to the case where each
respondent chooses exactly two options. When respondents may instead
choose a single option (or some combination of single and dual choices
within the dataset), care must be taken to ensure that the probabilities
of all possible response patterns sum to one. Simply adding
single-choice probabilities
[\[eq:Ps-single\]](#eq:Ps-single){reference-type="eqref"
reference="eq:Ps-single"} and dual-choice probabilities
[\[dual-choices\]](#dual-choices){reference-type="eqref"
reference="dual-choices"} side by side, without a joint model of the
process determining whether a respondent chooses one or two options,
will generally lead to probabilities that are not mutually exclusive and
do not sum to one.

A principled approach is to model a two-stage process:

1.  A selection mechanism (e.g. a binary or ordered choice model) that
    determines whether the respondent chooses one option or two
    (possibly depending on the utilities $V_{ij}$ or additional
    covariates), and

2.  Conditional on choosing one or two, a logit model for the choice
    over alternatives (single-choice or dual-choice probabilities as
    derived above).

This is conceptually similar to a Heckman-type selection model or a
two-part model, and it ensures that the joint probabilities over the
full space of outcomes (single vs. dual choices and which alternative(s)
are chosen) are properly normalised.